{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT 5310 Test #3 Due December 12, 2018 at 5:30pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predict the number of applications received using the other variables in the college data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(data.table)\n",
    "library(glmnet)\n",
    "library(glmnetUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "college <- fread('../data/College.csv',stringsAsFactors = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "Split the data set into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "college <- college %>% select(-V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=nrow(college)\n",
    "train_sample <- runif(n,0,1) > 1 - 0.75 #random uniform sample\n",
    "college_train <- college[train_sample,]\n",
    "college_test  <- college[!train_sample,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train <- college_train %>% select(-Apps) %>% as.matrix()\n",
    "y_train <- college_train %>% select( Apps) %>% as.matrix()\n",
    "x_test  <- college_test  %>% select(-Apps) %>% as.matrix()\n",
    "y_test  <- college_test  %>% select( Apps) %>% as.matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "Fit a linear model using least squares on the training set, and report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model <- lm(data = college_train, formula = Apps~.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Root Mean Squared Error on test dataset =  13687.5291913705'</span>"
      ],
      "text/latex": [
       "'Root Mean Squared Error on test dataset =  13687.5291913705'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Root Mean Squared Error on test dataset =  13687.5291913705'</span>"
      ],
      "text/plain": [
       "[1] \"Root Mean Squared Error on test dataset =  13687.5291913705\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual <- predict(linear_model,newdata = college_test) - college_test$Apps\n",
    "RMSE <- sqrt(sum(residual^2))\n",
    "paste('Root Mean Squared Error on test dataset = ',RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "Fit a ridge regression model on the training set, with λ chosen by cross-validation.  Report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian, :\n",
      "“NAs introduced by coercion”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian, : NA/NaN/Inf in foreign function call (arg 5)\n",
     "output_type": "error",
     "traceback": [
      "Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian, : NA/NaN/Inf in foreign function call (arg 5)\nTraceback:\n",
      "1. glmnet(x = x_train, y = y_train)",
      "2. glmnet.default(x = x_train, y = y_train)",
      "3. glmnet::glmnet(x, y, ...)",
      "4. elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian, \n .     alpha, nobs, nvars, jd, vp, cl, ne, nx, nlam, flmin, ulam, \n .     thresh, isd, intr, vnames, maxit)"
     ]
    }
   ],
   "source": [
    "ridge.mod = glmnet(x = x_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \n",
    "Consider the Boston housing data set, from the MASS library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "Based on this data set, provide an estimate for the population mean of medv.  Call this estimate  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "Provide an estimate of the standard error of   Interpret this result.\n",
    "Hint:  We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "Now estimate the standard error of  using the bootstrap.  How does this compare to your answer from (b)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. compare it to the results obtained using t.test(Boston$medv).\n",
    "\n",
    "Hint:  You can approximate a 95% confidence interval using the formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 6 Lab 1: Subset Selection Methods\n",
    "#Install package ISLR and leaps\n",
    "# Best Subset Selection\n",
    "\n",
    "library(ISLR)\n",
    "fix(Hitters)\n",
    "names(Hitters)\n",
    "dim(Hitters)\n",
    "sum(is.na(Hitters$Salary))\n",
    "Hitters=na.omit(Hitters)\n",
    "dim(Hitters)\n",
    "#reduces the rows from 322 to 263\n",
    "sum(is.na(Hitters))\n",
    "\n",
    "library(leaps)\n",
    "#regsubests from leaps provides a model search for the best subsets of each size up to nvmax exhaustive\n",
    "#is the default method\n",
    "\n",
    "regfit.full=regsubsets(Salary~.,Hitters)\n",
    "summary(regfit.full)\n",
    "regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)\n",
    "reg.summary=summary(regfit.full)\n",
    "names(reg.summary)\n",
    "\n",
    "reg.summary$rsq\n",
    "\n",
    "par(mfrow=c(2,2))\n",
    "plot(reg.summary$rss,xlab=\"Number of Variables\",ylab=\"RSS\",type=\"l\")\n",
    "plot(reg.summary$adjr2,xlab=\"Number of Variables\",ylab=\"Adjusted RSq\",type=\"l\")\n",
    "which.max(reg.summary$adjr2)\n",
    "#which.max returns 11 which we then include in the previous plot for ajr2\n",
    "points(11,reg.summary$adjr2[11], col=\"red\",cex=2,pch=20)\n",
    "\n",
    "plot(reg.summary$cp,xlab=\"Number of Variables\",ylab=\"Cp\",type='l')\n",
    "which.min(reg.summary$cp)\n",
    "#returns 10 which we then include in the previous Cp plot\n",
    "points(10,reg.summary$cp[10],col=\"red\",cex=2,pch=20)\n",
    "\n",
    "which.min(reg.summary$bic)\n",
    "#returns 6\n",
    "plot(reg.summary$bic,xlab=\"Number of Variables\",ylab=\"BIC\",type='l')\n",
    "points(6,reg.summary$bic[6],col=\"red\",cex=2,pch=20)\n",
    "\n",
    "\n",
    "plot(regfit.full,scale=\"r2\")\n",
    "plot(regfit.full,scale=\"adjr2\")\n",
    "plot(regfit.full,scale=\"Cp\")\n",
    "plot(regfit.full,scale=\"bic\")\n",
    "#Returns the coefficients from the model search of variables\n",
    "coef(regfit.full,6)\n",
    "\n",
    "# Forward and Backward Stepwise Selection\n",
    "\n",
    "regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method=\"forward\")\n",
    "summary(regfit.fwd)\n",
    "regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method=\"backward\")\n",
    "summary(regfit.bwd)\n",
    "#Exhaustive comes up with a slightly different 7 variable model than Forward and Backward\n",
    "coef(regfit.full,7)\n",
    "coef(regfit.fwd,7)\n",
    "coef(regfit.bwd,7)\n",
    "\n",
    "# Choosing Among Models\n",
    "\n",
    "set.seed(1)\n",
    "#Creates a vector of True and Falses with length equal to the rows of Hitters\n",
    "train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)\n",
    "test=(!train)\n",
    "\n",
    "#Find the best subsets using only the True rows from train\n",
    "regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)\n",
    "\n",
    "test.mat=model.matrix(Salary~.,data=Hitters[test,])\n",
    "\n",
    "#To calculate the validation error for each model\n",
    "val.errors=rep(NA,19)\n",
    "for(i in 1:19){\n",
    "   coefi=coef(regfit.best,id=i)\n",
    "   pred=test.mat[,names(coefi)]%*%coefi\n",
    "   val.errors[i]=mean((Hitters$Salary[test]-pred)^2)\n",
    "}\n",
    "val.errors\n",
    "\n",
    "which.min(val.errors)\n",
    "#Returns 10\n",
    "\n",
    "coef(regfit.best,10)\n",
    "\n",
    "\n",
    "predict.regsubsets=function(object,newdata,id,...){\n",
    "  form=as.formula(object$call[[2]])\n",
    "  mat=model.matrix(form,newdata)\n",
    "  coefi=coef(object,id=id)\n",
    "  xvars=names(coefi)\n",
    "  mat[,xvars]%*%coefi\n",
    "  }\n",
    "\n",
    "#Slightly different variables selected when using the full data set as compared to the training\n",
    "regfit.best=regsubsets(Salary~.,data=Hitters,nvmax=19)\n",
    "coef(regfit.best,10)\n",
    "\n",
    "#10-fold Cross-validation\n",
    "k=10\n",
    "set.seed(1)\n",
    "folds=sample(1:k,nrow(Hitters),replace=TRUE)\n",
    "cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))\n",
    "for(j in 1:k){\n",
    "  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)\n",
    "  for(i in 1:19){\n",
    "    pred=predict(best.fit,Hitters[folds==j,],id=i)\n",
    "    cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)\n",
    "    }\n",
    "  }\n",
    "mean.cv.errors=apply(cv.errors,2,mean)\n",
    "mean.cv.errors\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "plot(mean.cv.errors,type='b')\n",
    "#11 seems to have the smallest error\n",
    "\n",
    "reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)\n",
    "coef(reg.best,11)\n",
    "\n",
    "\n",
    "#Ridge Regression and the Lasso\n",
    "\n",
    "x=model.matrix(Salary~.,Hitters)[,-1]\n",
    "#[,-1] excludes the first column in the list of predictos\n",
    "y=Hitters$Salary\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "library(glmnet)\n",
    "grid=10^seq(10,-2,length=100)\n",
    "ridge.mod=glmnet(x,y,alpha=0,lambda=grid)\n",
    "dim(coef(ridge.mod))\n",
    "ridge.mod$lambda[50]\n",
    "coef(ridge.mod)[,50]\n",
    "sqrt(sum(coef(ridge.mod)[-1,50]^2))\n",
    "ridge.mod$lambda[60]\n",
    "coef(ridge.mod)[,60]\n",
    "sqrt(sum(coef(ridge.mod)[-1,60]^2))\n",
    "predict(ridge.mod,s=50,type=\"coefficients\")[1:20,]\n",
    "set.seed(1)\n",
    "train=sample(1:nrow(x), nrow(x)/2)\n",
    "test=(-train)\n",
    "y.test=y[test]\n",
    "ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)\n",
    "ridge.pred=predict(ridge.mod,s=4,newx=x[test,])\n",
    "mean((ridge.pred-y.test)^2)\n",
    "mean((mean(y[train])-y.test)^2)\n",
    "ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])\n",
    "mean((ridge.pred-y.test)^2)\n",
    "ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)\n",
    "mean((ridge.pred-y.test)^2)\n",
    "lm(y~x, subset=train)\n",
    "predict(ridge.mod,s=0,exact=T,type=\"coefficients\")[1:20,]\n",
    "set.seed(1)\n",
    "cv.out=cv.glmnet(x[train,],y[train],alpha=0)\n",
    "plot(cv.out)\n",
    "bestlam=cv.out$lambda.min\n",
    "bestlam\n",
    "ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])\n",
    "mean((ridge.pred-y.test)^2)\n",
    "out=glmnet(x,y,alpha=0)\n",
    "predict(out,type=\"coefficients\",s=bestlam)[1:20,]\n",
    "\n",
    "# The Lasso\n",
    "\n",
    "lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)\n",
    "plot(lasso.mod)\n",
    "set.seed(1)\n",
    "cv.out=cv.glmnet(x[train,],y[train],alpha=1)\n",
    "plot(cv.out)\n",
    "bestlam=cv.out$lambda.min\n",
    "lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])\n",
    "mean((lasso.pred-y.test)^2)\n",
    "out=glmnet(x,y,alpha=1,lambda=grid)\n",
    "lasso.coef=predict(out,type=\"coefficients\",s=bestlam)[1:20,]\n",
    "lasso.coef\n",
    "lasso.coef[lasso.coef!=0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Validation Set Approach\n",
    "\n",
    "\n",
    "Auto=read.csv(\"H:/STAT 5310/Auto.csv\",header=T,na.strings=\"?\")\n",
    "Auto=na.omit(Auto)\n",
    "dim(Auto)\n",
    "names(Auto)\n",
    "\n",
    "library(ISLR)\n",
    "set.seed(1)\n",
    "train=sample(392,196)\n",
    "lm.fit=lm(mpg~horsepower,data=Auto,subset=train)\n",
    "attach(Auto)\n",
    "mean((mpg-predict(lm.fit,Auto))[-train]^2)\n",
    "#26.14142\n",
    "\n",
    "lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)\n",
    "mean((mpg-predict(lm.fit2,Auto))[-train]^2)\n",
    "#19.82259\n",
    "\n",
    "lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)\n",
    "mean((mpg-predict(lm.fit3,Auto))[-train]^2)\n",
    "#19.78252\n",
    "\n",
    "set.seed(2)\n",
    "train=sample(392,196)\n",
    "lm.fit=lm(mpg~horsepower,subset=train)\n",
    "mean((mpg-predict(lm.fit,Auto))[-train]^2)\n",
    "#23.29559\n",
    "\n",
    "lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)\n",
    "mean((mpg-predict(lm.fit2,Auto))[-train]^2)\n",
    "#18.90124\n",
    "\n",
    "lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)\n",
    "mean((mpg-predict(lm.fit3,Auto))[-train]^2)\n",
    "#19.2574\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Leave-One-Out Cross-Validation\n",
    "\n",
    "glm.fit=glm(mpg~horsepower,data=Auto)\n",
    "coef(glm.fit)\n",
    "lm.fit=lm(mpg~horsepower,data=Auto)\n",
    "coef(lm.fit)\n",
    "\n",
    "library(boot)\n",
    "glm.fit=glm(mpg~horsepower,data=Auto)\n",
    "\n",
    "#cv.glm k-fold cross-validationn prediction error default is k=n \n",
    "cv.err=cv.glm(Auto,glm.fit)\n",
    "cv.err$delta\n",
    "cv.error=rep(0,5)\n",
    "for (i in 1:5){\n",
    " glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
    " cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]\n",
    " }\n",
    "cv.error\n",
    "#24.23151 19.24821 19.33498 19.42443 19.03321\n",
    "\n",
    "# k-Fold Cross-Validation\n",
    "\n",
    "#cv.glm k=10 10-fold cross validation \n",
    "set.seed(17)\n",
    "cv.error.10=rep(0,10)\n",
    "for (i in 1:10){\n",
    " glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
    " cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]\n",
    " }\n",
    "cv.error.10\n",
    "\n",
    "# The Bootstrap example from PowerPoint\n",
    "\n",
    "alpha.fn=function(data,index){\n",
    " X=data$X[index]\n",
    " Y=data$Y[index]\n",
    " return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))\n",
    " }\n",
    "alpha.fn(Portfolio,1:100)\n",
    "set.seed(1)\n",
    "alpha.fn(Portfolio,sample(100,100,replace=T))\n",
    "boot(Portfolio,alpha.fn,R=1000)\n",
    "\n",
    "# Estimating the Accuracy of a Linear Regression Model\n",
    "\n",
    "boot.fn=function(data,index)\n",
    " return(coef(lm(mpg~horsepower,data=data,subset=index)))\n",
    "\n",
    "boot.fn(Auto,1:392)\n",
    "set.seed(1)\n",
    "boot.fn(Auto,sample(392,392,replace=T))\n",
    "boot.fn(Auto,sample(392,392,replace=T))\n",
    "#Different results due to different with replacement samples\n",
    "\n",
    "boot(Auto,boot.fn,1000)\n",
    "summary(lm(mpg~horsepower,data=Auto))$coef\n",
    "#See how similar the results are bootstrap estimates vs. full sample\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
